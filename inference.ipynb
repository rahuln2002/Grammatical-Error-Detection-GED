{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore any warnings that may arise during execution\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importing the replicate library for interacting with the Replicate API (likely for using pre-trained models)\n",
    "import replicate\n",
    "\n",
    "# Import getpass to securely handle API keys\n",
    "from getpass import getpass\n",
    "\n",
    "# Import os for interacting with the operating system (e.g., file handling)\n",
    "import os\n",
    "\n",
    "# Import pandas for data manipulation (e.g., reading CSVs)\n",
    "import pandas as pd\n",
    "\n",
    "# Importing transformers for handling pre-trained BERT model and tokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Importing PyTorch for deep learning operations (e.g., model handling)\n",
    "import torch\n",
    "\n",
    "# Import OpenAI client for interacting with the OpenAI API\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Replicate API key as an environment variable, prompting the user to input it securely\n",
    "os.environ['REPLICATE_API_TOKEN'] = getpass(\"Enter Replicate API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable Python's HTTPS certificate verification (typically used for environments where certificate validation may be problematic)\n",
    "os.environ['PYTHONHTTPSVERIFY'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the OpenAI API key as an environment variable, prompting the user to input it securely\n",
    "os.environ['OPENAI_API_KEY'] = getpass(\"Enter OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client (ensure the OpenAI API key is set correctly in the environment)\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file \"test.csv\" into a DataFrame\n",
    "df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'sentence' column as a numpy array and store it in 'sentences'\n",
    "sentences = df.sentence.values\n",
    "\n",
    "# Extract the 'label' column as a numpy array and store it in 'labels'\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta Llama Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN)\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "# Loop through the first 500 sentences in the dataset\n",
    "for i in range(500):\n",
    "    # Run the model on the current sentence, asking the model to output '1' for grammatically correct and '0' for incorrect\n",
    "    output = replicate.run(\n",
    "        \"meta/meta-llama-3-70b-instruct\",  # Model being used\n",
    "        input={ \n",
    "            \"top_k\": 50,                    # Set the top-k sampling parameter\n",
    "            \"top_p\": 0.9,                   # Set the top-p (nucleus sampling) parameter\n",
    "            \"prompt\": f\"Give output as '1' when sentence is grammatically correct and '0' when sentence is grammatically incorrect. Don't correct the sentence. Just give '1' or '0' \\n Sentence: {sentences[i]}\",  # Construct the input prompt\n",
    "            \"max_tokens\": 512,               # Limit the response length\n",
    "            \"min_tokens\": 0,                 # Minimum tokens the model should output\n",
    "            \"temperature\": 0.6,              # Set the temperature for randomness\n",
    "            \"prompt_template\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",  # Prompt template\n",
    "            \"presence_penalty\": 1.15,        # Set the presence penalty to penalize new topics\n",
    "            \"frequency_penalty\": 0.2         # Set the frequency penalty to avoid repetition\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Print the model output ('1' or '0')\n",
    "    print(output[0], end=\" \")\n",
    "\n",
    "    # Compare model output with the actual label and update confusion matrix counts\n",
    "    if int(output[0]) == 1 and labels[i] == 1:  # Correctly identified grammatically correct sentence\n",
    "        TP += 1\n",
    "    elif int(output[0]) == 1 and labels[i] == 0:  # Incorrectly identified grammatically incorrect sentence\n",
    "        FP += 1\n",
    "    elif int(output[0]) == 0 and labels[i] == 1:  # Incorrectly identified grammatically correct sentence\n",
    "        FN += 1\n",
    "    elif int(output[0]) == 0 and labels[i] == 0:  # Correctly identified grammatically incorrect sentence\n",
    "        TN += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the loop, print the confusion matrix counts\n",
    "print(\"TP: \", TP)\n",
    "print(\"FP: \", FP)\n",
    "print(\"FN: \", FN)\n",
    "print(\"TN: \", TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI GPT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN)\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "# Loop through the first 500 sentences in the dataset\n",
    "for i in range(500):\n",
    "    # Send a request to OpenAI's GPT-4 model to classify the grammatical correctness of the current sentence\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",  # Model being used\n",
    "        messages=[ \n",
    "            {\n",
    "                \"role\": \"system\",  # System message to instruct the model\n",
    "                \"content\": \"Give '1' if sentence is grammatically correct and '0' if sentence is grammatically incorrect.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",  # User message containing the current sentence to evaluate\n",
    "                \"content\": sentences[i]  # The sentence to evaluate\n",
    "            }\n",
    "        ],\n",
    "        temperature=1,  # Temperature parameter for response randomness\n",
    "        max_tokens=256,  # Maximum tokens for the model's response\n",
    "        top_p=1,  # Top-p (nucleus sampling) to control randomness\n",
    "        frequency_penalty=0,  # No penalty for frequency of terms in the response\n",
    "        presence_penalty=0  # No penalty for the presence of new topics in the response\n",
    "    )\n",
    "\n",
    "    # Extract the model's output ('1' or '0') from the response and convert it to integer\n",
    "    output = int(str(response.choices[0].message.content)[0])  \n",
    "    print(f\"{i}.) {output}\", end=\" \")  # Print the output for each sentence\n",
    "\n",
    "    # Update the confusion matrix based on the model's prediction and the true label\n",
    "    if output == 1 and labels[i] == 1:  # Correctly identified grammatically correct sentence\n",
    "        TP += 1\n",
    "    elif output == 1 and labels[i] == 0:  # Incorrectly identified grammatically incorrect sentence\n",
    "        FP += 1\n",
    "    elif output == 0 and labels[i] == 1:  # Incorrectly identified grammatically correct sentence\n",
    "        FN += 1\n",
    "    elif output == 0 and labels[i] == 0:  # Correctly identified grammatically incorrect sentence\n",
    "        TN += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the loop, print the confusion matrix counts: TP, FP, FN, TN\n",
    "print(\"TP: \", TP)\n",
    "print(\"FP: \", FP)\n",
    "print(\"FN: \", FN)\n",
    "print(\"TN: \", TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where the fine-tuned model and tokenizer are stored\n",
    "output_dir = 'model_20k_bert_base_uncased_new_lang8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer from the saved fine-tuned model\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Load the fine-tuned model for sequence classification\n",
    "model_loaded = BertForSequenceClassification.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a GPU is available and set the device accordingly (CUDA for GPU, CPU if no GPU available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the selected device (GPU or CPU)\n",
    "model_loaded = model_loaded.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up fuction for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference function for grammar checking using a pre-trained BERT model\n",
    "def BertGrammarChecker(sentence):\n",
    "    # Store the input sentence for processing\n",
    "    sent = sentence\n",
    "\n",
    "    # Tokenize the input sentence and prepare it for BERT\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                    # Input sentence\n",
    "                            add_special_tokens=True,  # Add special tokens like [CLS] and [SEP]\n",
    "                            max_length=64,           # Limit the sentence length to 64 tokens\n",
    "                            pad_to_max_length=True,  # Pad to the maximum length (if necessary)\n",
    "                            return_attention_mask=True,  # Return attention mask (to ignore padding tokens)\n",
    "                            return_tensors='pt',      # Return PyTorch tensors\n",
    "                    )\n",
    "\n",
    "    # Extract input_ids and attention_mask from the encoded dictionary\n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "\n",
    "    # Move the tensors to the selected device (GPU or CPU)\n",
    "    input_id = input_id.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    # Disable gradient calculation for inference (to save memory and computation)\n",
    "    with torch.no_grad():\n",
    "        # Perform forward pass through the model to get the logits (predictions)\n",
    "        outputs = model_loaded(input_id, token_type_ids=None, attention_mask=attention_mask)\n",
    "\n",
    "        # Extract logits (predictions)\n",
    "        logits = outputs[0]\n",
    "        \n",
    "        # Get the index of the highest logit (most probable label)\n",
    "        index = logits.argmax()\n",
    "\n",
    "    # Return the predicted index (label)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on Saved Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters for True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN)\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "# Iterate over the first 500 sentences for inference\n",
    "for i in range(500):\n",
    "    # Get the predicted label (1 or 0) from the grammar checker function\n",
    "    lb = BertGrammarChecker(sentences[i])\n",
    "\n",
    "    # Compare the predicted label with the actual label (from the 'labels' list)\n",
    "    # Update the corresponding counter based on the prediction and ground truth\n",
    "    if lb == 1 and labels[i] == 1:\n",
    "        TP += 1  # True Positive: Correctly predicted as positive\n",
    "    elif lb == 1 and labels[i] == 0:\n",
    "        FP += 1  # False Positive: Incorrectly predicted as positive\n",
    "    elif lb == 0 and labels[i] == 1:\n",
    "        FN += 1  # False Negative: Incorrectly predicted as negative\n",
    "    elif lb == 0 and labels[i] == 0:\n",
    "        TN += 1  # True Negative: Correctly predicted as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the results for TP, FP, FN, and TN\n",
    "print(\"TP: \", TP)\n",
    "print(\"FP: \", FP)\n",
    "print(\"FN: \", FN)\n",
    "print(\"TN: \", TN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
